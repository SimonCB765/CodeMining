Currently only journal table format data is processable

Add intended contents of the two processed files (counts and binary)








*Code Mining Parameters*
[] == just take the one param set and generate the models
    only takes the first combination, so really just give it the optimal params and be done with it
[0, X] == perform the param optimisation using X fold CV, (defaulting to 10)
    Ideally X from here and X from the [Y>1, X] estimation should be the same, or there is a weaker relationship between the two
[1, ...] == perform holdout testing on all param combos
    records descent and is quick, so useful for checking whether param combos work at all or fail to converge before doing real testing
[Y>1, X] == nested CV (X defaults to Y) for estimating performance of whole training procedure


using the average of the internal CV fold performances to determine
	the best param setting makes sense when:
	1) the number of examples in each fold is equal (and misclas costs are =)
	2) the performance metric is somthing like accuracy or error rate
		that make sense when averaged. E.g. 50% accuracy on 10 examples
		and 50% on another ten is the same as 50% on all 20. When
		using something like G mean then evarging makes no sense


Nested
	split into K external folds
	for each external fold eCV:
		for each param setting:
			split examples not in eCV into K2 internal folds
			perform CV using K2 folds to evaluate model with given params
		choose best performing param setting over all internal folds
		create model training on all examples not in eCV and testing on those in eCV
	determine overall performance as some average of the performance of the K models tested on the eCV folds

*Image Generation*

Any result files beginning with ExternalFold_ were generated by the external fold performance testing.

Ambiguous.tsv - predictions for both models on the ambiguous examples.
BestParameters.tsv - the optimal parameters found.
BothModelDescentResults.tsv - the record of the SGD descent for each model.
BothModelPredictions.tsv - the predictions of both models on their training examples. Dashes are used for the second model on discarded examples.
Coefficients.tsv - the coefficients for both models.
HoldOutPerformance.tsv - the results of exploring parameter space using half dataset holdout testing.
ParameterOptimisation.tsv - the results of optimising the parameters using cross validation.